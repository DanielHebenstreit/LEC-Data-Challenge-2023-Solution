{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6112981,"sourceType":"datasetVersion","datasetId":3502893},{"sourceId":6183423,"sourceType":"datasetVersion","datasetId":3548771}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport numpy as np\n\nimport warnings\n\n# Ignore FutureWarning and UserWarning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-29T15:00:34.904163Z","iopub.execute_input":"2023-09-29T15:00:34.904985Z","iopub.status.idle":"2023-09-29T15:00:36.270082Z","shell.execute_reply.started":"2023-09-29T15:00:34.904943Z","shell.execute_reply":"2023-09-29T15:00:36.269326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We used a short compression function that improves the ram usage by downcasting datatypes that are too high (so no loss in information) and stored \n# it afterwards as \"ram_compressed_dataset.pkl\":\n\n#train['ID_engine'] = train['ID_engine'].astype('category')\n#train['cyl'] = train['cyl'].astype('uint8')\n#train['time'] = train['time'].astype('uint16')\n#train['load'] = train['load'].astype('float32')\n#train['ign_time'] = train['ign_time'].astype('float32')\n#train['knock_control'] = train['knock_control'].astype('float32')\n#train['knock_signal'] = train['knock_signal'].astype('float32')\n#train['noise_signal'] = train['noise_signal'].astype('float32')\n#train['temp_exh'] = train['temp_exh'].astype('float32')\n#train['case'] = train[\"case\"].replace({\"A\": 1, \"B\": 2, \"0\": 0}).astype('float16')\n#train.to_pickle('ram_compressed_dataset.pkl')\n\ntrain = pd.read_pickle('/input/lec-pickled/ram_compressed_dataset.pkl') \ntrain","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:00:36.271968Z","iopub.execute_input":"2023-09-29T15:00:36.272313Z","iopub.status.idle":"2023-09-29T15:00:51.251552Z","shell.execute_reply.started":"2023-09-29T15:00:36.272281Z","shell.execute_reply":"2023-09-29T15:00:51.250445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"import random\nrandom.seed(41)\n\n# A train/validation split of 90/10 but by Engine to not leak Engine level information\n\nval_stations = random.sample(list(train[\"ID_engine\"].unique()), int(len(train[\"ID_engine\"].unique()) * 0.1))\nval = train[train[\"ID_engine\"].isin(val_stations)]\nval = val.dropna(axis='rows')\nval\n\ntrain_stations = [obj for obj in list(train[\"ID_engine\"].unique()) if obj not in val_stations]\ntrain = train[train[\"ID_engine\"].isin(train_stations)]\ntrain = train.dropna(axis='rows')\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:00:51.252790Z","iopub.execute_input":"2023-09-29T15:00:51.253179Z","iopub.status.idle":"2023-09-29T15:01:09.441950Z","shell.execute_reply.started":"2023-09-29T15:00:51.253148Z","shell.execute_reply":"2023-09-29T15:01:09.436473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engeneering","metadata":{}},{"cell_type":"code","source":"# Slight Downsampling of the majority class (class 0)\n# We ditch some of the eninges that contain only cylinder with class 0  \n\ngrouped = train.groupby([\"ID_engine\"], observed=True).max()\nstations_without_case = grouped[grouped[\"case\"] == 0].index\nstations_with_case = grouped[grouped[\"case\"] != 0].index\n\nstations_without_case_to_keep = np.random.choice(list(stations_without_case), size=25, replace=False) # keep only 25 stations where all cylinder are always class 0\nstations_to_keep = list(stations_with_case) + list(stations_without_case_to_keep)\nprint(stations_to_keep)\nlen(stations_to_keep)\n\ntrain = train[train[\"ID_engine\"].isin(stations_to_keep)]\n\ndel grouped, stations_without_case, stations_with_case, stations_to_keep # del object to safe up some RAM\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:01:09.451855Z","iopub.execute_input":"2023-09-29T15:01:09.452834Z","iopub.status.idle":"2023-09-29T15:01:17.250479Z","shell.execute_reply.started":"2023-09-29T15:01:09.452744Z","shell.execute_reply":"2023-09-29T15:01:17.249354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We set these columns as target columns for our feature engineering \ntarget_columns = [\"knock_control\", \"knock_signal\", \"noise_signal\", \"temp_exh\", \"ign_time\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:01:17.252091Z","iopub.execute_input":"2023-09-29T15:01:17.252519Z","iopub.status.idle":"2023-09-29T15:01:17.258317Z","shell.execute_reply.started":"2023-09-29T15:01:17.252479Z","shell.execute_reply":"2023-09-29T15:01:17.257140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the difference between my cylinder and the mean cylinder (average of the respective engine) \n\ndef add_difference_to_mean_cylinder(data, time_station_mean_target_columns):\n    station_mean = data.groupby(['ID_engine', 'time'], observed=True)[time_station_mean_target_columns].mean().reset_index()\n    data = data.merge(station_mean, on=['ID_engine', 'time'], suffixes=('', '_mean'))\n\n    mean_columns = [f'{column}_mean' for column in time_station_mean_target_columns]\n    diff_to_mean_columns = [f'{column}_diff_to_mean' for column in time_station_mean_target_columns]\n\n    for diff_mean, target, mean_c in zip(diff_to_mean_columns, time_station_mean_target_columns, mean_columns):\n        data[diff_mean] = data[target] - data[mean_c]\n\n    data.drop(columns=mean_columns, inplace=True) # Drop mean column, we only want difference to mean columns\n    return data, mean_columns, diff_to_mean_columns\n        \ntime_station_mean_target_columns = target_columns \ntrain, mean_columns, diff_to_mean_columns =  add_difference_to_mean_cylinder(train, time_station_mean_target_columns)\ntrain # train now contains the difference to the engine mean feature","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:01:17.260011Z","iopub.execute_input":"2023-09-29T15:01:17.260745Z","iopub.status.idle":"2023-09-29T15:01:28.272981Z","shell.execute_reply.started":"2023-09-29T15:01:17.260708Z","shell.execute_reply":"2023-09-29T15:01:28.271800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a rolling mean over the target features and the difference to engine mean feature\n\ndef add_windows(data, columns_for_rolling_mean):\n    window_sizes = [30]\n\n    # Compute the rolling mean and add as new columns to the DataFrame\n    for window_size in window_sizes:\n        for column in columns_for_rolling_mean:\n            data[f'{column}_rolling_mean_{window_size}'] = data[column].rolling(window=window_size, min_periods=1).mean()\n            #data[f'{column}_rolling_std_{window_size}'] = data[column].rolling(window=window_size, min_periods=1).std()\n            \n    return data\n\ncolumns_for_rolling_mean = target_columns + diff_to_mean_columns\ntrain = add_windows(train, columns_for_rolling_mean)\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:01:28.274562Z","iopub.execute_input":"2023-09-29T15:01:28.274851Z","iopub.status.idle":"2023-09-29T15:01:44.296417Z","shell.execute_reply.started":"2023-09-29T15:01:28.274821Z","shell.execute_reply":"2023-09-29T15:01:44.295331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(list(train.columns)))\ntrain.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:01:44.297834Z","iopub.execute_input":"2023-09-29T15:01:44.300592Z","iopub.status.idle":"2023-09-29T15:01:44.307659Z","shell.execute_reply.started":"2023-09-29T15:01:44.300556Z","shell.execute_reply":"2023-09-29T15:01:44.306578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"non_included_features = [\"case\", \"ID_engine\", \"cyl\", ] # Features we want to exclude during training","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:01:44.309140Z","iopub.execute_input":"2023-09-29T15:01:44.310214Z","iopub.status.idle":"2023-09-29T15:01:44.320407Z","shell.execute_reply.started":"2023-09-29T15:01:44.310165Z","shell.execute_reply":"2023-09-29T15:01:44.319298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\nimport xgboost as xgb\nimport time\nfrom sklearn.metrics import matthews_corrcoef, hamming_loss\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = xgb.XGBClassifier(n_estimators=100, n_jobs=-1)\n\nmodel.fit(train.drop(non_included_features, axis=1), train[\"case\"])\n\ndel train","metadata":{"execution":{"iopub.status.busy":"2023-09-29T15:01:44.324451Z","iopub.execute_input":"2023-09-29T15:01:44.324919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib\nmodel_filename = \"xgboost_model.joblib\"\njoblib.dump(model, model_filename) # save the model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val, _, _ = add_difference_to_mean_cylinder(val, time_station_mean_target_columns)\nval = add_windows(val, columns_for_rolling_mean)\n\nval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val[\"predictions\"] = model.predict(val.drop(non_included_features, axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the longest sequence of consecutive appearances of class 1 or 2\n\ndef longest_consecutive(sequence, value):\n    max_count = 0\n    current_count = 0\n\n    for num in sequence:\n        if num == value:\n            current_count += 1\n            max_count = max(max_count, current_count)\n        else:\n            current_count = 0\n\n    return max_count\n\n# Group by \"ID_engine\" and \"cyl\" and calculate longest consecutive sequence of 1 or 2 in \"predictions\"\n#val.groupby([\"ID_engine\", \"cyl\"], observed=True)[\"predictions\"].apply(lambda x: longest_consecutive(x, 1))\n#val.groupby([\"ID_engine\", \"cyl\"], observed=True)[\"predictions\"].apply(lambda x: longest_consecutive(x, 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Post processing: setting predictions on cylinder level where class 1/2 appears less than 5 times to class 0 (reduce False Positives for Event Classification) \n\ndef custom_function(group):\n    if (group['predictions'] == 1).sum() < 5:\n        group.loc[group['predictions'] == 1, 'predictions'] = 0\n    if (group['predictions'] == 2).sum() < 5:\n        group.loc[group['predictions'] == 2, 'predictions'] = 0\n        \n    return group\n\nval_grouped = val.groupby([\"ID_engine\", \"cyl\"], observed=True).apply(custom_function)\n\nval_grouped.reset_index(drop=True, inplace=True)\nval = val_grouped\nval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scores and visualisation of the validation set predictions\n\ndef calculate_and_print_scores(val):\n    stats_df = val.groupby([\"ID_engine\", \"cyl\"], observed=True).apply(lambda group: pd.Series({\n        \"case_0\": (group[\"case\"] == 0).sum(),\n        \"case_1\": (group[\"case\"] == 1).sum(),\n        \"case_2\": (group[\"case\"] == 2).sum(),\n        \"pred_0\": (group[\"predictions\"] == 0).sum(),\n        \"pred_1\": (group[\"predictions\"] == 1).sum(),\n        \"pred_2\": (group[\"predictions\"] == 2).sum(),\n        \"con_1\": longest_consecutive(group[\"predictions\"], 1),\n        \"con_2\": longest_consecutive(group[\"predictions\"], 2)\n    })).reset_index()\n\n    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n        print(stats_df.groupby([\"ID_engine\", \"cyl\"], observed=True).max())\n    \n    \n    val_by_cyl = val.groupby([\"ID_engine\", \"cyl\"], observed=True).max()\n    binary_mcc = matthews_corrcoef(val_by_cyl[\"predictions\"] != 0, val_by_cyl[\"case\"] != 0) \n\n    # Calculate the multiclass Matthews correlation coefficient (MCC)\n    multiclass_mcc = matthews_corrcoef(val_by_cyl[\"predictions\"], val_by_cyl[\"case\"]) \n\n    # Calculate the normalized Hamming distance\n    hamming_distance = hamming_loss(val[\"predictions\"], val['case']) \n\n    # Weighted average calculation\n    overall_score = (0.6 * binary_mcc) + (0.2 * multiclass_mcc) + (0.2 *  (1 - hamming_distance))\n\n    # Print the evaluation metrics\n    print(\"Binary MCC:\", binary_mcc)\n    print(\"Multiclass MCC:\", multiclass_mcc)\n    print(\"Normalized Hamming Distance:\", hamming_distance)\n    print(\"Overall Score:\", overall_score)\n    \n    cm = confusion_matrix(val_by_cyl['case'], val_by_cyl[\"predictions\"])\n    confusion_df = pd.DataFrame(cm, index=['True 0', 'True 1', 'True 2'], columns=['Pred 0', 'Pred 1', 'Pred 2'])\n\n    # Here, we display the confusion matrix on Event Classification Level\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(confusion_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted Class\")\n    plt.ylabel(\"True Class\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n    \n    cm = confusion_matrix(val['case'], val[\"predictions\"])\n    confusion_df = pd.DataFrame(cm, index=['True 0', 'True 1', 'True 2'], columns=['Pred 0', 'Pred 1', 'Pred 2'])\n\n    # Here, we display the confusion matrix on per timestep prediction level\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(confusion_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted Class\")\n    plt.ylabel(\"True Class\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n    \n    return binary_mcc, multiclass_mcc, hamming_distance, overall_score\n\ncalculate_and_print_scores(val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}